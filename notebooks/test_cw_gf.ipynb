{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ca70b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686379f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def download_if_missing(url: str, dest: Path | str, chunk_size: int = 8192) -> Path:\n",
    "    \"\"\"\n",
    "    Download *url* to *dest* unless the file is already present.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        HTTP or HTTPS address of the remote file.\n",
    "    dest : pathlib.Path or str\n",
    "        Target filename (absolute or relative).  Parent directories will be\n",
    "        created automatically.\n",
    "    chunk_size : int, optional\n",
    "        Number of bytes to read per network iteration.  Defaults to 8 KiB.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pathlib.Path\n",
    "        Path to the existing or newly downloaded file.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * Uses :pymeth:`pathlib.Path.exists` to avoid unnecessary downloads.  [oai_citation:4‡PyTutorial](https://pytutorial.com/check-file-existence-with-python-pathlibexists/?utm_source=chatgpt.com)\n",
    "    * Streams the response with ``requests.get(..., stream=True)`` to handle\n",
    "      large weights efficiently.  [oai_citation:5‡Real Python](https://realpython.com/python-download-file-from-url/?utm_source=chatgpt.com)\n",
    "    * Displays a live progress bar powered by *tqdm*.  [oai_citation:6‡proxiesapi.com](https://proxiesapi.com/articles/downloading-binary-files-with-python-requests?utm_source=chatgpt.com)\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from pathlib import Path\n",
    "    >>> ckpt = download_if_missing(\n",
    "    ...     \"https://huggingface.co/epigen/cellwhisperer/resolve/main/cellwhisperer_clip_v1.ckpt\",\n",
    "    ...     Path(\"~/models/cellwhisperer_clip_v1.ckpt\"))\n",
    "    >>> ckpt\n",
    "    PosixPath('/home/you/models/cellwhisperer_clip_v1.ckpt')\n",
    "    \"\"\"\n",
    "    dest = Path(dest).expanduser().resolve()\n",
    "    if dest.exists():\n",
    "        return dest  # nothing to do\n",
    "\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get(\"Content-Length\", 0))\n",
    "        with (\n",
    "            tqdm(total=total, unit=\"B\", unit_scale=True, desc=dest.name) as bar,\n",
    "            open(dest, \"wb\") as f,\n",
    "        ):\n",
    "            for chunk in r.iter_content(chunk_size):\n",
    "                if chunk:  # filter out keep-alive chunks\n",
    "                    f.write(chunk)\n",
    "                    bar.update(len(chunk))\n",
    "\n",
    "    return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d90f7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/mengerj/repos/adata_hf_datasets/notebooks/cellwhisperer_jointemb_v1.ckpt')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CKPT = \"/Users/mengerj/repos/adata_hf_datasets/notebooks/cellwhisperer_jointemb_v1.ckpt\"\n",
    "URL = \"https://medical-epigenomics.org/papers/schaefer2024/data/models/cellwhisperer_clip_v1.ckpt\"\n",
    "download_if_missing(URL, CKPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ef887d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wandb'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcellwhisperer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_io\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_cellwhisperer_model\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# ----------------- load model on GPU -------------------\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# IMPORTANT: cache=False to avoid repeated GPU loading on Mac MPS\u001b[39;00m\n\u001b[32m      4\u001b[39m pl_model, tokenizer, transcriptome_processor = load_cellwhisperer_model(CKPT, \u001b[38;5;28meval\u001b[39m=\u001b[38;5;28;01mTrue\u001b[39;00m, cache=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/adata_hf_datasets/external/CellWhisperer/src/cellwhisperer/utils/model_io.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcellwhisperer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjointemb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TranscriptomeTextDualEncoderProcessor\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcellwhisperer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjointemb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcellwhisperer_lightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     TranscriptomeTextDualEncoderLightning,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcellwhisperer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_path_from_name\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/adata_hf_datasets/external/CellWhisperer/src/cellwhisperer/jointemb/cellwhisperer_lightning.py:20\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcellwhisperer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjointemb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     TranscriptomeTextDualEncoderConfig,\n\u001b[32m     15\u001b[39m     TranscriptomeTextDualEncoderModel,\n\u001b[32m     16\u001b[39m     CLIPOutput,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcli\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OptimizerCallable, LRSchedulerCallable\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Artifact, Table\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Union, Dict, List\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'wandb'"
     ]
    }
   ],
   "source": [
    "from cellwhisperer.utils.model_io import load_cellwhisperer_model\n",
    "\n",
    "# ----------------- load model on GPU -------------------\n",
    "# IMPORTANT: cache=False to avoid repeated GPU loading on Mac MPS\n",
    "pl_model, tokenizer, transcriptome_processor = load_cellwhisperer_model(\n",
    "    CKPT, eval=True, cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4604f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "\n",
    "adata_path = \"HIHA_pp.h5ad\"\n",
    "adata = anndata.read_h5ad(adata_path)\n",
    "adata.X = adata.layers[\"counts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40fb6b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adata_hf_datasets import InitialEmbedder\n",
    "\n",
    "# cw_model_path: directory or file passed to GeneformerModel.from_pretrained(...)\n",
    "ie = InitialEmbedder(\n",
    "    method=\"cw-geneformer\",\n",
    "    embedding_dim=512,\n",
    "    cw_model_path=\"/Users/mengerj/repos/adata_hf_datasets/external/Geneformer_v1/geneformer-12L-30M\",\n",
    "    # Optional:\n",
    "    # processor_kwargs={\"nproc\": 6, \"emb_label\": [\"sample_name\"]},\n",
    "    # model_config={\"emb_mode\": \"cell\", \"emb_layer\": -1, \"forward_batch_size\": 16},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83790773",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie.prepare(adata)  # loads processor and CW Geneformer model\n",
    "X = ie.embed(\n",
    "    adata=adata,\n",
    "    obsm_key=\"X_cw_geneformer\",\n",
    "    batch_size=16,  # optional: overrides model config forward_batch_size\n",
    "    chunk_size=512,  # forwarded to processor (tokenization)\n",
    "    target_sum=10_000,  # forwarded to processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bc7d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write X to file\n",
    "import pickle\n",
    "\n",
    "with open(\"cw_gf_HIHA.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c43ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "\n",
    "adata_path = \"HIHA_pp.h5ad\"\n",
    "adata = anndata.read_h5ad(adata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7410f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "adata.obs[\"n_counts\"] = adata.obs[\"total_counts\"]\n",
    "adata.obs[\"sample_index\"] = np.arange(adata.n_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47adddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adata_hf_datasets import InitialEmbedder\n",
    "\n",
    "# cw_model_path: directory or file passed to GeneformerModel.from_pretrained(...)\n",
    "ie = InitialEmbedder(\n",
    "    method=\"geneformer-v1\",\n",
    "    geneformer_v1_root=\"/Users/mengerj/repos/adata_hf_datasets/external/Geneformer_v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa0a177b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.modeling_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mie\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m X = ie.embed(\n\u001b[32m      3\u001b[39m     adata=adata,\n\u001b[32m      4\u001b[39m     obsm_key=\u001b[33m\"\u001b[39m\u001b[33mX_geneformer-v1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/adata_hf_datasets/src/adata_hf_datasets/embed/initial_embedder.py:2893\u001b[39m, in \u001b[36mInitialEmbedder.prepare\u001b[39m\u001b[34m(self, adata, adata_path, **prepare_kwargs)\u001b[39m\n\u001b[32m   2891\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m adata_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2892\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mUsing file path: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, adata_path)\n\u001b[32m-> \u001b[39m\u001b[32m2893\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata\u001b[49m\u001b[43m=\u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43madata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/adata_hf_datasets/src/adata_hf_datasets/embed/initial_embedder.py:1223\u001b[39m, in \u001b[36mGeneformerEmbedder.prepare\u001b[39m\u001b[34m(self, adata, adata_path, do_tokenization, **kwargs)\u001b[39m\n\u001b[32m   1220\u001b[39m \u001b[38;5;66;03m# Patch transformers to make HybridCache available at top level\u001b[39;00m\n\u001b[32m   1221\u001b[39m \u001b[38;5;28mself\u001b[39m._patch_transformers_hybrid_cache()\n\u001b[32m-> \u001b[39m\u001b[32m1223\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeneformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TranscriptomeTokenizer\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m adata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1226\u001b[39m     \u001b[38;5;66;03m# If adata object is provided, we still need to save it temporarily to use the efficient methods\u001b[39;00m\n\u001b[32m   1227\u001b[39m     logger.warning(\n\u001b[32m   1228\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAnnData object provided directly. For memory efficiency, consider providing adata_path instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1229\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/adata_hf_datasets/.venv/lib/python3.12/site-packages/geneformer/__init__.py:17\u001b[39m\n\u001b[32m     14\u001b[39m ENSEMBL_DICTIONARY_FILE_30M = Path(\u001b[34m__file__\u001b[39m).parent / \u001b[33m\"\u001b[39m\u001b[33mgene_dictionaries_30m/gene_name_id_dict_gc30M.pkl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m ENSEMBL_MAPPING_FILE_30M = Path(\u001b[34m__file__\u001b[39m).parent / \u001b[33m\"\u001b[39m\u001b[33mgene_dictionaries_30m/ensembl_mapping_dict_gc30M.pkl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     collator_for_classification,\n\u001b[32m     19\u001b[39m     emb_extractor,\n\u001b[32m     20\u001b[39m     in_silico_perturber,\n\u001b[32m     21\u001b[39m     in_silico_perturber_stats,\n\u001b[32m     22\u001b[39m     pretrainer,\n\u001b[32m     23\u001b[39m     tokenizer,\n\u001b[32m     24\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcollator_for_classification\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     26\u001b[39m     DataCollatorForCellClassification,\n\u001b[32m     27\u001b[39m     DataCollatorForGeneClassification,\n\u001b[32m     28\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01memb_extractor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EmbExtractor, get_embs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/adata_hf_datasets/.venv/lib/python3.12/site-packages/geneformer/emb_extractor.py:28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TOKEN_DICTIONARY_FILE\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m perturber_utils \u001b[38;5;28;01mas\u001b[39;00m pu\n\u001b[32m     30\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# extract embeddings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/adata_hf_datasets/.venv/lib/python3.12/site-packages/geneformer/perturber_utils.py:14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, load_from_disk\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     BertForMaskedLM,\n\u001b[32m     17\u001b[39m     BertForSequenceClassification,\n\u001b[32m     18\u001b[39m     BertForTokenClassification,\n\u001b[32m     19\u001b[39m     BitsAndBytesConfig,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/adata_hf_datasets/.venv/lib/python3.12/site-packages/peft/__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m0.18.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[32m     19\u001b[39m     AutoPeftModel,\n\u001b[32m     20\u001b[39m     AutoPeftModelForCausalLM,\n\u001b[32m     21\u001b[39m     AutoPeftModelForFeatureExtraction,\n\u001b[32m     22\u001b[39m     AutoPeftModelForQuestionAnswering,\n\u001b[32m     23\u001b[39m     AutoPeftModelForSeq2SeqLM,\n\u001b[32m     24\u001b[39m     AutoPeftModelForSequenceClassification,\n\u001b[32m     25\u001b[39m     AutoPeftModelForTokenClassification,\n\u001b[32m     26\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig, PromptLearningConfig\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmapping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     PEFT_TYPE_TO_CONFIG_MAPPING,\n\u001b[32m     30\u001b[39m     PEFT_TYPE_TO_MIXED_MODEL_MAPPING,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     inject_adapter_in_model,\n\u001b[32m     34\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/adata_hf_datasets/.venv/lib/python3.12/site-packages/peft/auto.py:32\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     22\u001b[39m     AutoModel,\n\u001b[32m     23\u001b[39m     AutoModelForCausalLM,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     AutoTokenizer,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpeft_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     33\u001b[39m     PeftModel,\n\u001b[32m     34\u001b[39m     PeftModelForCausalLM,\n\u001b[32m     35\u001b[39m     PeftModelForFeatureExtraction,\n\u001b[32m     36\u001b[39m     PeftModelForQuestionAnswering,\n\u001b[32m     37\u001b[39m     PeftModelForSeq2SeqLM,\n\u001b[32m     38\u001b[39m     PeftModelForSequenceClassification,\n\u001b[32m     39\u001b[39m     PeftModelForTokenClassification,\n\u001b[32m     40\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_CONFIG_NAME\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mother\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_file_exists_on_hf_hub\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/adata_hf_datasets/.venv/lib/python3.12/site-packages/peft/peft_model.py:42\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QuestionAnsweringModelOutput, SequenceClassifierOutput, TokenClassifierOutput\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtuners\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlora\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_alora_offsets_for_forward, get_alora_offsets_for_generate\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtuners\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtuners_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseTuner, BaseTunerLayer\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AuxiliaryTrainingWrapper\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/adata_hf_datasets/.venv/lib/python3.12/site-packages/peft/tuners/__init__.py:15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01madalora\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AdaLoraConfig, AdaLoraModel\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01madaption_prompt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AdaptionPromptConfig, AdaptionPromptModel\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mboft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BOFTConfig, BOFTModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/adata_hf_datasets/.venv/lib/python3.12/site-packages/peft/tuners/adalora/__init__.py:18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bnb_4bit_available, is_bnb_available\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_peft_method\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AdaLoraConfig\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgptq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SVDQuantLinear\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AdaLoraLayer, RankAllocator, SVDLinear\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/adata_hf_datasets/.venv/lib/python3.12/site-packages/peft/tuners/adalora/config.py:19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass, field\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtuners\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlora\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftType\n\u001b[32m     23\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAdaLoraConfig\u001b[39;00m(LoraConfig):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/adata_hf_datasets/.venv/lib/python3.12/site-packages/peft/tuners/lora/__init__.py:23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgptq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPTQLoraLinear\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Conv2d, Conv3d, Embedding, Linear, LoraLayer, ParamWrapper\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraModel\n\u001b[32m     26\u001b[39m __all__ = [\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mArrowConfig\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mConv2d\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minitialize_lora_eva_weights\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m ]\n\u001b[32m     45\u001b[39m register_peft_method(name=\u001b[33m\"\u001b[39m\u001b[33mlora\u001b[39m\u001b[33m\"\u001b[39m, config_cls=LoraConfig, model_cls=LoraModel, is_mixed_compatible=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/adata_hf_datasets/.venv/lib/python3.12/site-packages/peft/tuners/lora/model.py:26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_layers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GradientCheckpointingLayer\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bnb_4bit_available, is_bnb_available\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtuners\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtuners_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     30\u001b[39m     BaseTuner,\n\u001b[32m     31\u001b[39m     BaseTunerLayer,\n\u001b[32m     32\u001b[39m     replicate_layers,\n\u001b[32m     33\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers.modeling_layers'"
     ]
    }
   ],
   "source": [
    "ie.prepare(adata)\n",
    "X = ie.embed(adata=adata, obsm_key=\"X_geneformer-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4368b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "\n",
    "adata = ad.read_zarr(\n",
    "    \"../data/RNA/processed_with_emb/train/cellxgene_pseudo_bulk_10k/train/chunk_0.zarr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa124931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 8113 × 17530\n",
       "    obs: 'age', 'assay', 'assay_ontology_term_id', 'cell_type', 'cell_type_ontology_term_id', 'cluster_name', 'development_stage', 'development_stage_ontology_term_id', 'disease', 'disease_ontology_term_id', 'is_primary_data', 'lobe', 'organism', 'organism_ontology_term_id', 'orig.ident', 'self_reported_ethnicity', 'self_reported_ethnicity_ontology_term_id', 'sex', 'sex_ontology_term_id', 'suspension_type', 'tissue', 'tissue_ontology_term_id', 'based_on_n_cells', 'abstract', 'dataset_title', 'Sort_id', 'celltype_annotation', 'is_maternal_contaminant', 'original_pub', 'predicted_doublets', 'sample', 'sample_long', 'scvi_clusters', 'Age', 'Amyloid', 'Braak', 'Cell.Types', 'PMI', 'RIN', 'SORT', 'Sample.ID', 'dissection', 'roi', 'sample_id', 'supercluster_term', 'batch', 'cell_cycle', 'cell_description', 'BMI', 'cds', 'donor_region', 'group', 'hash_cluster', 'hash_label', 'hashed', 'lineage', 'pct_counts_hb', 'phase', 'treatment', 'type', 'subcluster_id', 'brain_region', 'cluster_label', 'cortical_area', 'lamina', 'author_cell_type', 'author_cluster_label', 'hca_data_portal_cellsuspension_uuid', 'hca_data_portal_donor_uuid', 'age_group', 'alignment_software', 'bmi_group', 'breast_density', 'broad_cell_type', 'cell_state', 'donor_BMI_at_collection', 'donor_living_at_sample_collection', 'donor_menopausal_status', 'library_uuid', 'mapped_reference_annotation', 'mapped_reference_assembly', 'procedure_group', 'sample_derivation_process', 'sample_preservation_method', 'sample_source', 'sample_uuid', 'sequencing_platform', 'suspension_depleted_cell_types', 'suspension_derivation_process', 'suspension_dissociation_reagent', 'suspension_dissociation_time', 'suspension_percent_cell_viability', 'suspension_uuid', 'tissue_location', 'library_starting_quantity', 'Celltype', 'Cluster', 'celltype', 'seurat_clusters', 'Layer', 'Type', 'age group', 'biosample_id', 'donor_BMI', 'donor_times_pregnant', 'family_history_breast_cancer', 'tyrer_cuzick_lifetime_risk', 'ParticipantID', 'Phase', 'cellSubtype', 'genotype_1', 'gravidityParity', 'menopause', 'numberOfBirths', 'numberOfPregnancies', 'surgeryType', 'CellType', 'Experiment', 'Selection', 'Short_Sample', 'compartment', 'suspension_depletion_factors', 'suspension_enriched_cell_types', 'suspension_enrichment_factors', 'chemistry', 'new_celltype', 'diet', 'digest', 'reported_diseases', 'typeSample', 'author_Cause_of_Death', 'author_Donor_PMI', 'author_FACS_Classification', 'Time', 'donor_time', 'organ', 'ALK_mutation', 'BRAF_mutation', 'EGFR_mutation', 'ERBB2_mutation', 'KRAS_mutation', 'ROS_mutation', 'TP53_mutation', 'ann_coarse', 'ann_fine', 'cell_type_major', 'cell_type_neutro', 'cell_type_neutro_coarse', 'cell_type_predicted', 'cell_type_tumor', 'dataset', 'doublet_status', 'ever_smoker', 'origin', 'origin_fine', 'platform', 'study', 'tumor_stage', 'uicc_stage', 'cell_type_fine', 'cell_type_intermediate', 'cell_type_main', 'initial_clustering', 'interval_death_symptoms_onset_days', 'intubation_days', 'pmi_h', 'recorded_ethnicity', 'recorded_race', 'C-reactive protein (mg per dL)', 'Chest X-ray', 'Comorbidity', 'Consciousness', 'Disease group', 'Heart rate (BPM)', 'Hospital day', 'Lymphocyte per microL (%)', 'Monocyte prt microL (%)', 'NEWS score', 'Neutrophil per microL (%)', 'O2 saturation', 'O2 supplement', 'Respiratory rate (BPM)', 'Sample ID', 'Severity', 'Systolic BP', 'Temperature', 'Treatment', 'WBC per microL', 'class', 'diabetes_history', 'disease_category', 'eGFR', 'experiment_id', 'hypertension', 'library_id', 'percent.cortex', 'percent.medulla', 'region', 'specimen', 'subclass.l1', 'subclass.l2', 'Collection.ID', 'Genotype', 'Location', 'PCW', 'Pool', 'broad_extfig7A_cell.labels', 'cell.labels', 'lanes', 'CRL', 'Donor_nb', 'Enrichment_fraction', 'Purification', 'Sample', 'cell_name', 'cell_name_detailed', 'cell_type_group', 'doublet_scores_observed_cells', 'condition.l1', 'condition.l2', 'condition.long', 'experiment', 'id', 'library', 'pagoda_k100_infomap_coembed', 'region.l1', 'region.l2', 'state', 'state.l2', 'structure', 'subclass.full', 'subclass.l3', 'tissue_type', 'HbA1c', 'glucose_SI', 'insulin_content', 'louvain_anno_broad', 'louvain_anno_fine', 'BraakStage', 'SampleBatch', 'SampleID', 'clusterAssignment', 'clusterCellType', 'initialClusterAssignments', 'seurat.clusters', 'Processing_Cohort', 'ct_cov', 'disease_state', 'ind_cov', 'TP', 'bonf_pval', 'cryopreserved', 'dig_protocol', 'enrichment', 'high_mito', 'is_doublet', 'lineageSomatic', 'location', 'low_ncounts', 'low_ncounts_high_mito', 'trimester', 'days_since_hospitalized', 'days_since_onset', 'dsm_severity_score', 'dsm_severity_score_group', 'ever_admitted_to_icu', 'material_type', 'outcome', 'severity', 'timepoint', 'author_tissue', 'epilepsy_duration', 'epilepsy_frequency', 'hemisphere', 'smoking', 'RNA_snn_res.1.2', 'cell_label', 'leiden_scVI', 'Annotation', 'Batch', 'Group', 'SARS-CoV-2_PCR', 'Sample_ID', 'Ethnicity', 'Gender', 'Pack Years', 'Phenograph_cluster', 'ProcedureType', 'Procedure_Type', 'Race', 'Smoking Status', 'Stage at Dx', 'Tissue Site', 'Tissue Type', 'Treatment Status', 'cell_lineage', 'histology', 'hta_donor_id', 'hta_id', 'sample_name', 'sample_number', 'ChainStatus', 'DoubletFinderPrediction', 'IGH_D_CALL', 'IGH_FUNCTIONAL', 'IGH_IN_FRAME', 'IGH_JUNCTION_LENGTH', 'IGH_J_CALL', 'IGH_STOP', 'IGH_V_CALL_GENOTYPED', 'IGK_C_Gene', 'IGK_FullLength', 'IGK_Productive', 'IGK_VDJ_Gene', 'IGL_C_Gene', 'IGL_FullLength', 'IGL_Productive', 'IGL_VDJ_Gene', 'ISOTYPE', 'Lineage', 'ScrubletPrediction', 'Status', 'obstructive_sleep_apnea', 'recurrent_tonsillitis', 'Population', 'resolution_0.1', 'resolution_0.2', 'resolution_0.3', 'resolution_0.4', 'resolution_0.5', 'resolution_0.75', 'resolution_1', 'resolution_2', 'resolution_3', 'resolution_4', 'resolution_5', 'area', 'area_long', 'slab', 'celltype_predictions', 'coarse_annot', 'dev_age', 'number_of_individuals_multiplexed', 'origin_M_F', 'technology', 'Age_group', 'Diagnosis', 'Fraction', 'category', 'Dataset', 'BroadCellType', 'COVID-19 Condition', 'Cell.class_reannotated', 'Cell.group', 'disease_general', 'disease_original', 'tissue_original', 'author_cluster', 'author_cluster_name', 'author_sample', 'author_type', 'channel', 'free_annotation', 'magnetic.selection', 'preparation.site', 'astro', 'customclassif', 'percent_hb', 'seq_folder', '10XBatch', 'AgeGroup', 'CauseOfDeath_category', 'ID', 'SequencingPool', 'donor_cause_of_death', 'tissue_handling_interval', 'cell_type_label', 'condition', 'ega_sample_alias', 'filtered_out_cells', 'infomap_cluster', 'brca_status', 'dissociation_minutes', 'donor_age', 'ethnicity_original', 'level0', 'level1', 'level2', 'parity', 'pred_spikein', 'prob_spikein', 'processing_date', 'risk_status', 'sampleID', 'sample_type', 'sample_type_coarse', 'tissue_condition', 'packyears', 'HTAN_Biospecimen_ID', 'HTAN_Participant_ID', 'cell_type_coarse', 'cell_type_general', 'cell_type_med', 'clusters', 'histo', 'procedure', 'author_RNA_snn_res.0.5', 'author_RNA_snn_res.1', 'author_RNA_snn_res.2', 'author_sample_group', 'author_sample_id', 'cell_type_original', 'major_labl', 'patient_region_id', 'expCond', 'GenotypingRef_H18_30_002', 'Binary Stage', 'BiopsyType', 'Broad cell type', 'Cell type', 'CellCycle Phase', 'Stage', 'doublet_id', 'Admission', 'DPS', 'DTF', 'Ventilated', 'cell.type.coarse', 'cell.type.fine', 'singler', 'Project', 'broad_celltype', 'Re-annotation', 'COVID_severity', 'COVID_status', 'Cell_type_annotation_level1', 'Cell_type_annotation_level2', 'Cell_type_annotation_level3', 'Ethnicity_inferred', 'First_symptoms_collection_interval', 'First_symptoms_hospitalisation_interval', 'Kit_version', 'Positive_test_collection_interval', 'Smoker', 'n_counts_total_sarscov2', 'broad_fig1_cell.labels', 'stage', 'cardiacevent_72h', 'cluster', 'fever_symptoms', 'gastrointestinal_symptoms', 'preexisting_diabetes', 'preexisting_heartdisease', 'preexisting_hypertension', 'preexisting_immunocompromisedcondition', 'preexisting_kidneydisease', 'preexisting_lungdisease', 'rank', 'respiratory_symptoms', 'time_point', 'who_d0', 'who_d28', 'who_d3', 'who_d7', 'who_max', 'ATRX', 'EGFR', 'MET', 'MGMT', 'PDGFR', 'PTEN', 'TERT', 'annotation_level_1', 'annotation_level_2', 'annotation_level_3', 'author', 'celltype_original', 'chr1p19q', 'gbmap', 'method', 'p53', 'sector', 'activation', 'bh_pval', 'cell_type_corrected_final', 'scrublet_cluster_score', 'scrublet_score', 'file', 'sort', 'clusters_fine', 'histology_subtype', 'log1p_n_genes_by_counts', 'n_genes', 'n_genes_by_counts', 'age.days.GA', 'age.order', 'cell_source', 'cell_states', 'clusters.high.res', 'clusters.low.res', 'clusters.res.2', 'clusters.res.3', 'integrated_snn_res.0.1', 'integration.groups', 'size.CRL', 'size.NRL', 'annotation', 'broad_type', 'summaryDescription', 'big_cluster', 'Ctype', 'Ctype_Final', 'PresumedFusion', 'detected_nfusion', 'detected_nwt', 'projCtype', 'cd4cd8_status', 'status', 'stimulation_status', 'author_cell_type1', 'author_cell_type2', 'author_cell_type_broad', 'author_cell_type_stroma', 'lesion', 'Age_week', 'Major_cell_type', 'annotated_organ', 'annotated_tissue', 'Annotated Cell Sets', 'Anterior vs Posterior', 'Study', 'Tooth #/Region', 'Tooth Type', 'final_cluster_labels', 'Pan_cell_type', 'annotation_V2', 'inferred state', 'BCR_ChainCombination', 'BCR_ISOTYPE', 'TCR_c_gene', 'TCR_d_gene', 'TCR_j_gene', 'TCR_umis', 'TCR_v_gene', 'final_cluster', 'patient_group', 'TCR/BCR', 'cell_type_in_paper', 'Majority_voting_CellTypist', 'Majority_voting_CellTypist_high', 'Manually_curated_celltype', 'Predicted_labels_CellTypist', 'cell types', 'Age (years)', 'analysis_group', 'disease_setting', 'therapy', 'IGH_UMIS', 'IGK_UMIS', 'IGL_UMIS', 'enriched_cell_types', 'Smoking', 'Source', 'n_counts', 'process', 'donor_type', 'facility', 'flushed', 'sangerID', 'natural_language_annotation', 'transcriptome_weights', 'annotation_weights', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_38029_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'outlier', 'mt_outlier', 'sample_index'\n",
       "    var: 'ensembl_id', 'gene_name', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n",
       "    uns: 'hvg', 'log1p'\n",
       "    obsm: 'X_cw-geneformer', 'X_gs', 'X_hvg', 'X_pca', 'X_scvi_fm', 'natural_language_annotation_replicates'\n",
       "    layers: 'counts', 'replicate_1', 'replicate_2', 'replicate_3', 'replicate_4', 'replicate_5'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# visualise the embedding using umap\n",
    "key_to_plot = \"X_cw-geneformer\"\n",
    "# perform leiden clustering\n",
    "del adata.uns[\"neighbors\"]\n",
    "# Clean up any old leiden color palettes\n",
    "if \"leiden_colors\" in adata.uns:\n",
    "    del adata.uns[\"leiden_colors\"]\n",
    "# Compute neighbors graph using the specified embedding with cosine distance\n",
    "sc.pp.neighbors(adata, use_rep=key_to_plot, metric=\"cosine\")\n",
    "# Verify which representation was used (stored in neighbors params)\n",
    "print(f\"Neighbors computed from: {adata.uns['neighbors']['params']['use_rep']}\")\n",
    "print(f\"Embedding shape: {adata.obsm[key_to_plot].shape}\")\n",
    "sc.tl.leiden(adata)\n",
    "# Compute UMAP for visualization (uses the neighbors graph computed above)\n",
    "umap_key = f\"X_umap_{key_to_plot}\"  # Store with explicit key\n",
    "sc.tl.umap(adata, neighbors_key=\"neighbors\")\n",
    "sc.pl.umap(adata, color=[\"leiden\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
