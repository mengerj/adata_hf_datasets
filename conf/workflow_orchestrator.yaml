# Workflow Orchestrator Configuration
# This config contains settings for the workflow orchestrator only
# Dataset configs should be specified separately via command line

# =============================================================================
# WORKFLOW MODE CONFIGURATION
# =============================================================================
workflow:
  # Execution backend: "slurm" (default, submit via SSH + SLURM) or "local" (run on this machine)
  execution_mode: local

  # Transfer mode configuration
  # Set to true to use local scratch storage with transfers between clusters (faster processing, slower transfers)
  # Set to false to use shared filesystem directly (slower processing, no transfer overhead)
  # This allows performance comparison between local scratch + transfers vs shared filesystem
  enable_transfers: false

  # SSH connection settings for CPU cluster
  # Using SSH aliases from ~/.ssh/config (handles ProxyJump automatically)
  cpu_login:
    host: "imbi13" # SSH alias that resolves through biom5
    user: "menger" # Use the user from SSH config

  # SSH connection settings for GPU cluster
  # Using SSH aliases from ~/.ssh/config (handles ProxyJump automatically)
  gpu_login:
    host: "imbi_gpu_H100" # SSH alias that resolves through biom5
    user: "menger" # Use the user from SSH config

  # SLURM partition names (from actual cluster configuration)
  cpu_partition: "slurm" # CPU partition on imbi13
  gpu_partition: "gpu" # GPU partition on imbi_gpu_H100

  # Node constraint for CPU jobs (optional)
  # If set, all CPU jobs will be constrained to run on this specific node
  # GPU jobs are not affected by this setting
  # Must be a node that exists in the cpu_partition (slurm)
  # Available nodes in slurm partition: imbi3, imbi4, imbi5, imbi6, imbi7, imbi11
  # Example: "imbi3" or null to allow any node
  cpu_node: imbi3

  # Job polling interval (seconds)
  poll_interval: 3600

  # Timeout for job completion (seconds, 0 = no timeout)
  job_timeout: 0

  # Output directory configuration
  # Separate paths for local and SLURM execution modes
  # The actual workflow directory will be: {output_directory}/{date}/workflow_{job_id}/
  local_output_directory: "./outputs" # For local execution
  slurm_output_directory: "/home/menger/git/adata_hf_datasets/outputs" # For SLURM execution

  # Project directory configuration
  # Separate paths for local and SLURM execution modes
  # All commands will `cd` into this directory before running
  local_project_directory: "." # For local execution (relative to where script is run)
  slurm_project_directory: "/home/menger/git/adata_hf_datasets" # For SLURM execution

  # Local backend settings (effective only when execution_mode == "local")
  # Maximum number of parallel workers for local embedding array tasks
  local_max_workers: 2
  # Enable running GPU embedding locally (off by default to avoid OOM on laptops)
  local_enable_gpu: true

  # Base data directories depending on backend
  # Used as default when dataset config doesn't set base_file_path explicitly
  local_base_file_path: "./data/RNA"
  slurm_base_file_path: "/scratch/local/menger/data/RNA"

  # Virtual environment path (relative to project_directory) used by SLURM scripts
  venv_path: ".venv"

# =============================================================================
# ORCHESTRATOR SETTINGS
# =============================================================================
# Force flag to skip config synchronization validation
force: false

# Dataset config name to use (will be overridden by command line)
dataset_config_name: null
