# Workflow Orchestrator Configuration
# This config contains settings for the workflow orchestrator only
# Dataset configs should be specified separately via command line

# =============================================================================
# WORKFLOW MODE CONFIGURATION
# =============================================================================
workflow:
  # Execution backend: "slurm" (default, submit via SSH + SLURM) or "local" (run on this machine)
  execution_mode: slurm

  # SSH connection settings for CPU cluster
  # Using SSH aliases from ~/.ssh/config (handles ProxyJump automatically)
  cpu_login:
    host: "imbi121-ukf" # SSH alias that resolves through biom5
    user: "mengerj" # Use the user from SSH config

  # SSH connection settings for GPU cluster
  # Using SSH aliases from ~/.ssh/config (handles ProxyJump automatically)
  gpu_login:
    host: "imbi_gpu_H100" # SSH alias that resolves through biom5
    user: "menger" # Use the user from SSH config

  # SLURM partition names (from actual cluster configuration)
  cpu_partition: "compute" # CPU partition on imbi121-ukf
  gpu_partition: "gpu" # GPU partition on imbi_gpu_H100

  # Execution host: "cpu" or "gpu"
  # Determines which cluster/hardware to use for ALL workflow steps (download, preprocessing, embedding, dataset creation)
  # When set to "cpu": All steps run on the CPU cluster using cpu_partition and cpu_node (if specified)
  # When set to "gpu": All steps run on the GPU cluster using gpu_partition and gpu_node (if specified)
  # Note: For embedding in GPU mode, the master coordination job runs on CPU cluster, but array jobs run on GPU
  #host: gpu # Options: "cpu" or "gpu"

  # Node constraint for CPU jobs (optional)
  # If set, all CPU jobs will be constrained to run on this specific node
  # Must be a node that exists in the cpu_partition (slurm)
  # Available nodes in slurm partition: imbi3, imbi4, imbi5, imbi6, imbi7, imbi11
  # Example: "imbi3" or null to allow any node
  cpu_node: null

  # Node constraint for GPU jobs (optional)
  # If set, GPU embedding jobs will be constrained to run on this specific node
  # If null, any available GPU node will be used
  # Example: "gpu-node-1" or null to allow any GPU node
  gpu_node: null

  # Job polling interval (seconds)
  # How often the master job checks if a submitted SLURM job has finished
  # Default: 600 seconds (10 minutes)
  # Lower values = more frequent checks (faster detection of completion, more SSH overhead)
  # Higher values = less frequent checks (less SSH overhead, slower detection of completion)
  poll_interval: 120 # Check every 2 minutes

  # Timeout for job completion (seconds, 0 = no timeout)
  # Maximum time to wait for a job to complete before timing out
  # If 0, wait indefinitely (no timeout)
  # If > 0, timeout after this many seconds
  job_timeout: 0 # No timeout (wait indefinitely)

  # Output directory configuration
  # Separate paths for local and SLURM execution modes
  # The actual workflow directory will be: {output_directory}/{date}/workflow_{job_id}/
  local_output_directory: "./outputs" # For local execution
  slurm_output_directory: "/h/mengerj/repos/adata_hf_datasets/outputs" # For SLURM execution

  # Project directory configuration
  # Separate paths for local and SLURM execution modes
  # All commands will `cd` into this directory before running
  local_project_directory: "." # For local execution (relative to where script is run)
  slurm_project_directory: "/h/mengerj/repos/adata_hf_datasets" # For SLURM execution

  # Local backend settings (effective only when execution_mode == "local")
  # Maximum number of parallel workers for local embedding array tasks
  local_max_workers: 2
  # Enable running GPU embedding locally (off by default to avoid OOM on laptops)
  local_enable_gpu: true

  # Base data directories depending on backend
  # Used as default when dataset config doesn't set base_file_path explicitly
  local_base_file_path: "./data/RNA"
  slurm_base_file_path: "/scratch/local/menger/data/RNA"

  # Virtual environment paths (relative to project_directory)
  # Separate paths for local and SLURM execution modes
  local_venv_path: ".venv" # For local execution
  slurm_venv_path: "/scratch/local/menger/venvs/adata_hf_datasets" # For SLURM execution

# =============================================================================
# ORCHESTRATOR SETTINGS
# =============================================================================
# Force flag to skip config synchronization validation
force: false

# Dataset config name to use (will be overridden by command line)
dataset_config_name: null
