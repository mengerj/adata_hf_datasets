# Clean default configuration for all datasets
# This config contains dataset-specific parameters and execution parameters

# =============================================================================
# DATASET METADATA
# =============================================================================
dataset:
  name: null # e.g., "cellxgene_pseudo_bulk_35k"
  description: null # Human-readable description
  download_url: null # URL to download the raw data
  full_name: null # Name for the full downloaded file (e.g., "cellxgene_pseudo_bulk_full")

# =============================================================================
# COMMON KEYS (defined once, used across workflows)
# =============================================================================
# Batch key for batch-aware processing (used in preprocessing, embedding, and dataset creation)
batch_key: null # e.g., "dataset_title", "batch", "donor"

# Annotation key for cell type/annotation (used in dataset creation and consolidation)
annotation_key: null # e.g., "cell_type", "celltype", "annotation"

# Caption/description key for natural language descriptions (used in preprocessing and dataset creation)
caption_key: null # e.g., "natural_language_annotation", "description", null if not available

# Instrument key for technical metadata (used in preprocessing)
instrument_key: null # e.g., "assay", "instrument", null if not available

# Other biological labels for consolidation and plotting (e.g., tissue, disease, age)
other_bio_labels: [] # e.g., ["tissue", "disease", "age", "sex"]

# =============================================================================
# DATASET-LEVEL CONFIGURATION
# =============================================================================
# Split dataset into train/val (true for training datasets, false for test datasets)
# This setting applies to both preprocessing and dataset creation
split_dataset: true # true = training dataset with train/val split, false = test dataset with single "all" split

# =============================================================================
# DOWNLOAD CONFIGURATION (dataset-specific)
# =============================================================================
download:
  enabled: false # Whether to download the dataset (set to true if download_url is provided)
  execution_location: local # Execution location: local | cpu | gpu (where this step runs)
  subset_size: null # Number of observations to include in subset (null = no subsetting)
  seed: 42 # Random seed for reproducible subsetting
  stratify_keys: null # List of column names to stratify by (will be auto-generated from common keys)
  preserve_proportions: false # Whether to preserve proportions when stratifying
  validate: true # Validate downloaded file format
  keep_full_file: true # Keep the full downloaded file for future subsetting
  venv_path: null # Virtual environment path (relative to project_directory). If null, uses workflow.venv_path or ".venv"
  memory_gb: 60

# =============================================================================
# PREPROCESSING CONFIGURATION (dataset-specific overrides)
# =============================================================================
preprocessing:
  enabled: true # Whether to run preprocessing step
  # Execution location: local | cpu | gpu (where this step runs)
  execution_location: local
  # Dataset-specific processing parameters
  memory_gb: 60
  n_chunks: null
  enable_plotting: false
  min_cells: 20 # Minimum number of cells a gene should be expressed in
  min_genes: 200 # Minimum number of genes a cell should express
  n_top_genes: 5000 # Number of top genes to select
  count_layer_key: null # Key in adata.layers with raw counts. If not present, adata.raw.X is checked.

  # Category consolidation parameters
  category_threshold: 5 # Remove categories with fewer than this many samples
  remove_low_frequency: false # Whether to remove low frequency categories

  # Layer management
  layers_to_delete: null # List of layer names to delete from adata.layers (e.g., ["replicate_1", "replicate_2"])

  # Bimodal splitting (for specific datasets)
  split_bimodal: false
  bimodal_col: null

  # SRA metadata fetching (dataset-specific)
  skip_sra_fetch: false
  sra_max_retries: 3
  sra_continue_on_fail: true
  sra_chunk_size: null
  sra_extra_cols: [null]

  # Execution parameters (accessed by preprocessing script)
  chunk_size: 200000 # Processing chunk size
  output_format: "zarr" # Output format preference
  geneformer_pp: true # Geneformer preprocessing
  split_dataset: ${split_dataset} # Reference dataset-level split configuration
  train_split: 0.9 # Train/validation split ratio (only used if split_dataset=true)
  random_seed: 42 # Random seed for splits

  # Chunking parameters
  random_chunking: null # If true, use random chunking (even if batch_key provided); if false, use batch-based (requires batch_key); if null, auto-detect (random if no batch_key, batch-based if batch_key provided)
  chunk_random_seed: 42 # Random seed for random chunking (if null, uses system random)
  metrics_of_interest: # Quality control metrics
    - "n_genes_by_counts"
    - "total_counts"
    - "pct_counts_mt"

  # Virtual environment configuration
  venv_path: null # Virtual environment path (relative to project_directory). If null, uses workflow.venv_path or ".venv"

# =============================================================================
# EMBEDDING PREPARATION CONFIGURATION (dataset-specific overrides)
# =============================================================================
embedding_preparation:
  # Whether to run embedding preparation step
  enabled: true # Set to false to skip embedding preparation
  # Execution location: local | cpu | gpu (where this step runs)
  execution_location: local

  # Preparation-specific parameters
  prepare_only: true # Always true for preparation step
  mode: "slurm" # Use CPU partition for preparation

  # SLURM resource allocation
  memory_gb: 60 # Memory allocation in GB (default: 60GB). Increase for larger datasets (e.g., 80, 120)

  # Execution parameters (accessed by embedding preparation script)
  input_format: "auto" # Input format detection
  output_format: "zarr" # Output format preference
  overwrite: false # Whether to overwrite existing preparation files
  chunk_rows: 16384 # Number of cells written per chunk when streaming into Zarr
  # Method-specific parameters
  batch_size: 128 # Used by geneformer

  init_kwargs: null

  # Embedding dimensions (dataset-specific)
  embedding_dim_map:
    scvi_fm: 50 # Can't be changed as the pretrained scvi model is fixed.
    geneformer: 768
    pca: 50
    hvg: 512
    gs: 3936
    gs10k: 10000
    geneformer-v1: 512
    cw-geneformer: 512

  # Virtual environment configuration
  venv_path: null # Virtual environment path (relative to project_directory). If null, uses workflow.venv_path or ".venv"

# =============================================================================
# CPU EMBEDDING CONFIGURATION (dataset-specific overrides)
# =============================================================================
embedding_cpu:
  # Whether to run CPU embedding step
  enabled: true # Set to false to skip CPU embedding
  # Execution location: local | cpu | gpu (where this step runs)
  execution_location: local

  # CPU embedding methods to apply
  methods: ["hvg", "pca", "scvi_fm"] # CPU-only methods: ["scvi_fm", "pca", "hvg"]

  # SLURM resource allocation
  memory_gb: 60 # Memory allocation in GB (default: 60GB). Increase for larger datasets (e.g., 80, 120)

  # Method-specific parameters
  batch_size: 128 # Used by geneformer

  # Initialization kwargs for embedders (optional)
  # These kwargs are passed to all embedders; unused kwargs are ignored by embedders that don't need them
  init_kwargs: null

  # Embedding dimensions (dataset-specific)
  embedding_dim_map:
    scvi_fm: 50 # Can't be changed as the pretrained scvi model is fixed.
    geneformer: 768
    pca: 50
    hvg: 512
    gs: 3936
    gs10k: 10000
    geneformer-v1: 512
    cw-geneformer: 512

  # Execution parameters (accessed by embedding script)
  input_format: "auto" # Input format detection
  output_format: "zarr" # Output format preference
  overwrite: false # Whether to overwrite existing embeddings
  chunk_rows: 16384 # Number of cells written per chunk when streaming into Zarr

  # Virtual environment configuration
  venv_path: null # Virtual environment path (relative to project_directory). If null, uses workflow.venv_path or ".venv"

# =============================================================================
# GPU EMBEDDING CONFIGURATION (dataset-specific overrides)
# =============================================================================
embedding_gpu:
  # Whether to run GPU embedding step
  enabled: true # Set to false to skip GPU embedding
  # Execution location: local | cpu | gpu (where this step runs)
  execution_location: local

  # GPU embedding methods to apply
  methods: ["geneformer"] # GPU-required methods: ["geneformer"]

  # SLURM resource allocation
  memory_gb: 60 # Memory allocation in GB (default: 60GB). Increase for larger datasets (e.g., 80, 120)

  # Method-specific parameters
  batch_size: 128 # Used by geneformer

  # Initialization kwargs for embedders (optional)
  # These kwargs are passed to all embedders; unused kwargs are ignored by embedders that don't need them
  # Relative paths are resolved relative to PROJECT_DIR environment variable
  init_kwargs:
    # For geneformer: root directory of the Geneformer repository
    geneformer_root: "external/Geneformer" # Relative paths are resolved relative to PROJECT_DIR
    # For cw-geneformer: path to the model directory
    cw_model_path: "external/Geneformer_v1/geneformer-12L-30M" # Relative paths are resolved relative to PROJECT_DIR
  #   # Or use absolute path: "/absolute/path/to/geneformer-12L-30M"
  #   processor_kwargs:
  #     nproc: 4
  #     emb_label: ["sample_name"]
  #   model_config:
  #     emb_mode: "cell"
  #     emb_layer: -1
  #     forward_batch_size: 16

  # Embedding dimensions (dataset-specific)
  embedding_dim_map:
    scvi_fm: 50 # Can't be changed as the pretrained scvi model is fixed.
    geneformer: 768
    pca: 50
    hvg: 512
    gs: 3936
    gs10k: 10000
    geneformer-v1: 512
    cw-geneformer: 512

  # Execution parameters (accessed by embedding script)
  input_format: "auto" # Input format detection
  output_format: "zarr" # Output format preference
  overwrite: false # Whether to overwrite existing embeddings
  chunk_rows: 16384 # Number of cells written per chunk when streaming into Zarr

  # Virtual environment configuration
  venv_path: null # Virtual environment path (relative to project_directory). If null, uses workflow.venv_path or ".venv"

# =============================================================================
# DATASET CREATION CONFIGURATION (dataset-specific overrides)
# =============================================================================
dataset_creation:
  enabled: true # Whether to run dataset creation step
  # Execution location: local | cpu | gpu (where this step runs)
  execution_location: local
  # Dataset structue
  dataset_name: ${dataset.name}
  raw_data_link: ${dataset.download_url}
  data_description: ${dataset.description}
  split_dataset: ${split_dataset} # Reference dataset-level split configuration
  dataset_format: "multiplets" # "multiplets" for training, "single" for test datasets

  # Sentence generation parameters
  sentence_keys:
    - "sample_id_og" # Will be added to obs within the script
    - "cell_sentence"
  #    - "semantic_true"
  #    - "semantic_similar"

  # Required embeddings (dataset-specific)
  required_obsm_keys: ["X_pca", "X_scvi_fm"]

  # Cell sentence generation parameters
  gene_name_column: "gene_name" # Column in .var containing gene names
  include_label_prob: 0
  cs_length: 4096
  caption_keys: ["caption_key"]

  # Caption and negative sampling
  negatives_per_sample: 2
  resolve_negatives: false # If true, resolve negative indices to content (only works with single sentence_key)

  # Output configuration
  output_dir: "data/hf_datasets"
  push_to_hub: true
  # base_repo_id is auto-detected from HuggingFace token or CLI login
  private: true # Whether to make the HuggingFace Hub dataset private (true) or public (false)

  # Attribute removal (before upload)
  # Remove specified attributes from zarr/h5ad files to reduce file size
  # For zarr: attributes are deleted directly from disk (efficient, no loading)
  # For h5ad: files are loaded, modified, and saved back
  # Use "/" to separate nested keys (e.g., "uns/log1p/base")
  attributes_to_remove: null # List of attributes to remove, e.g., ["raw", "uns/log1p/base"]

  # Nextcloud upload (dataset-specific)
  use_nextcloud: false
  nextcloud_config:
    url: "NEXTCLOUD_URL"
    username: "NEXTCLOUD_USER"
    password: "NEXTCLOUD_PASSWORD"
    remote_path: ""

  # Zenodo upload (dataset-specific)
  use_zenodo: false
  zenodo_config:
    sandbox: false # Set to true to use Zenodo sandbox (https://sandbox.zenodo.org) instead of production
    # Environment variables required:
    # - ZENODO_TOKEN: Production Zenodo access token (when sandbox: false)
    # - ZENODO_SANDBOX_TOKEN: Sandbox Zenodo access token (when sandbox: true)
    # Create tokens at: https://zenodo.org/account/settings/applications/ (production)
    #                  https://sandbox.zenodo.org/account/settings/applications/ (sandbox)

  # Virtual environment configuration
  venv_path: null # Virtual environment path (relative to project_directory). If null, uses workflow.venv_path or ".venv"
